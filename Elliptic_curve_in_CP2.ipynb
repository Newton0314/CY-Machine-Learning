{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FIm6mUYtGC5"
      },
      "source": [
        "# MLGeometry guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCemOECptGC6"
      },
      "source": [
        "This introduction demonstrates how to use MLGeometry to:\n",
        "1. Generate a hypersurface.\n",
        "2. Build a bihomogeneous neural network.\n",
        "3. Use the model to compute numerical Calabi-Yau metrics with the embedding method.\n",
        "4. Plot $\\eta$ on a rational curve."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the package (on Colab)"
      ],
      "metadata": {
        "id": "ilHaPYnkEi-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install MLGeometry-tf"
      ],
      "metadata": {
        "id": "5pVEmL9vErvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edfe7169-276c-4766-d2bc-b143cca7c080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: MLGeometry-tf in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: tensorflow-probability[tf] in /usr/local/lib/python3.12/dist-packages (from MLGeometry-tf) (0.25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from MLGeometry-tf) (1.13.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from MLGeometry-tf) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (2.9.0.post0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->MLGeometry-tf) (1.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (1.17.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (3.1.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (0.1.9)\n",
            "Requirement already satisfied: tensorflow>=2.16 in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (2.19.0)\n",
            "Requirement already satisfied: tf-keras>=2.16 in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (2.19.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (25.2.10)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.5.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree->tensorflow-probability[tf]->MLGeometry-tf) (25.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyFvWKNmtGC7"
      },
      "source": [
        "## Configure imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhqc2oMWtGC8"
      },
      "source": [
        "Import tensorflow_probability to use the L-BFGS optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doBhWopntGC9"
      },
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import keras\n",
        "import csv\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJJvGaCNtGC-"
      },
      "outputs": [],
      "source": [
        "import MLGeometry as mlg\n",
        "from MLGeometry import bihomoNN as bnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9F3AKqPtGC_"
      },
      "source": [
        "Import the libraries to plot the $\\eta$ on the rational curve (see the last section):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9LWSTH3tGC_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd1zG07TtGDA"
      },
      "source": [
        "## Set a random seed (optional)\n",
        "Some random seed might be bad for numerical calulations. If there are any errors during the training, you may want to try a different seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5Rz0lXmtGDB"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjB84Ln1tGDB"
      },
      "source": [
        "## Define a hypersurface\n",
        "First define a set of coordinates and a function as sympy symbols:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwhQynDBtGDB"
      },
      "outputs": [],
      "source": [
        "# Elliptic curves as CY 1-fold in CP^2\n",
        "z0, z1, z2 = sp.symbols('z0, z1, z2')\n",
        "\n",
        "Z = [z0, z1, z2]\n",
        "\n",
        "fermat = z0**3 + z1**3 + z2**3\n",
        "\n",
        "zeta5 = np.exp(2*np.pi*1j/5)\n",
        "\n",
        "t = 0.1\n",
        "\n",
        "# Dwork family (indexed by t) in CP^3\n",
        "f_x = fermat + t*z0*z1*z2  # X_t\n",
        "\"\"\"\n",
        "#f_z = t*fermat + (z1**4 + z2**4 + z3**4) + z0*(z1**3 + z2**3 + z3**3) + z0**3 * (z1 + z2 + z3)  # Z_t (wrong equation)\n",
        "f_z = t*fermat + (z1**4 + z2**4 + z3**4) + z0*(z1**3 + z2**3 + z3**3) + z0**2 * (z1**2 + z2**2 + z3**2)  # Z_t\n",
        "#f_w = t*fermat + (z0**2 + z1*z2 + z3**2)*(z1**2 + z2*z3 + z0**2)  # W_t\n",
        "f_w = t*fermat + z0*(z0**3 + z1**3 + z2**3 + z3**3) # W_t\n",
        "f_w2 = t*fermat + z0*(z1**3 + z2**3 + z3**3)\n",
        "\"\"\"\n",
        "f_v = t*fermat + z0*z1*z2 # V_t\n",
        "\n",
        "f = f_v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTm215kftGDC"
      },
      "source": [
        "Then define a hypersurface as a collection of points which solve the equation f = 0, using the `Hypersurface` class in the `mlg.hypersurface` module. The parameter n_pairs is the number of random pairs of points used to form the random lines in $\\mathbf{CP}^{N+1}$. Then we take the intersections of those random lines and the hypersurface. By Bezout's theorem, each line intersects the hypersurface in precisely d points where d is the number of homogeneous coordinates. So the total number of points is d * n_pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz1Vi4Y2tGDC"
      },
      "outputs": [],
      "source": [
        "n_pairs = 1280\n",
        "HS_train = mlg.hypersurface.Hypersurface(Z, f, n_pairs)\n",
        "HS_test = mlg.hypersurface.Hypersurface(Z, f, n_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE981r2ctGDC"
      },
      "source": [
        "The Hypersurface class will take care of the patchwork automatically. Let's use the `list_patches` function to check the number of points on each patch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zyw84dftGDC",
        "outputId": "3da09169-342a-4236-e106-0c8a9cfd4be4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Patches: 3\n",
            "Points on patch 1 : 1270\n",
            "Points on patch 2 : 1286\n",
            "Points on patch 3 : 1284\n"
          ]
        }
      ],
      "source": [
        "HS_train.list_patches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JtfwpmFtGDD"
      },
      "source": [
        "You can also invoke this method on one of the patches to check the distribution on the subpatches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fMZAyKutGDD",
        "outputId": "27677dd0-6c1b-4e70-c4ee-24063365d3af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Patches: 2\n",
            "Points on patch 1 : 644\n",
            "Points on patch 2 : 626\n"
          ]
        }
      ],
      "source": [
        "HS_train.patches[0].list_patches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8p5D9ZWtGDE"
      },
      "source": [
        "The Hypersurface class contains some symbolic and numerical methods as well, which will be introduced elsewhere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u1oIOLytGDE"
      },
      "source": [
        "## Training with Tensorflow\n",
        "The following steps are similar to a regular Tensorflow training process.\n",
        "### Generate datasets\n",
        "The `mlg.tf_dataset.generate_dataset` function converts a hypersurface to a Tensorflow Dataset, which has four componets: the points on the hypersurface, the volume form $\\small \\Omega \\wedge \\bar\\Omega$, the mass reweighting the points distribution and the restriction which restricts the Kähler metric to a subpatch. The restriction contains an extra linear transformation so that points on different affine patches can all be processed in one call. It is also possible to generate a dataset only on one affine patch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGq-mKdDtGDE"
      },
      "outputs": [],
      "source": [
        "train_set = mlg.tf_dataset.generate_dataset(HS_train)\n",
        "test_set = mlg.tf_dataset.generate_dataset(HS_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHP3ExA1tGDG"
      },
      "source": [
        "Shuffle and batch the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1AX_cm_tGDG"
      },
      "outputs": [],
      "source": [
        "train_set = train_set.shuffle(HS_train.n_points).batch(128)\n",
        "test_set = test_set.shuffle(HS_test.n_points).batch(128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7Mga5fdtGDG"
      },
      "source": [
        "Let's look at what is inside a dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kwHPcIFHtGDH",
        "outputId": "d8afd4c8-938f-4f1e-a8f7-a4c9b77d994d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([-0.06983887-0.06357283j  1.        +0.j          0.42707974-0.5707137j ], shape=(3,), dtype=complex64)\n"
          ]
        }
      ],
      "source": [
        "points, Omega_Omegabar, mass, restriction = next(iter(train_set))\n",
        "print(points[1])\n",
        "pts = points.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kVSGnnktGDH"
      },
      "source": [
        "### Build a bihomogeneous neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV_JRrERtGDI"
      },
      "source": [
        "The `mlg.bihomoNN` module provides the necessary layers (e.g. `Bihomogeneous` and `Dense` ) to construct the Kähler potential with a bihomogeneous neural network. Here is an example of a two-hidden-layer network (k = 4) with 70 and 100 hidden units:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9mE_YLltGDI"
      },
      "outputs": [],
      "source": [
        "@keras.saving.register_keras_serializable(package=\"MLGeometry\")\n",
        "class Kahler_potential(tf.keras.Model):\n",
        "    def __init__(self, trainable=True, dtype='float32', **kwargs):\n",
        "        super(Kahler_potential, self).__init__(trainable=trainable, dtype=dtype, **kwargs)\n",
        "        # The first layer transforms the complex points to the bihomogeneous form.\n",
        "        # The number of the outputs is d^2, where d is the number of coordinates.\n",
        "        self.bihomogeneous = bnn.Bihomogeneous(d=len(Z))\n",
        "        self.layer1 = bnn.SquareDense(len(Z)**2, 70, activation=tf.square)\n",
        "        self.layer2 = bnn.SquareDense(70, 100, activation=tf.square)\n",
        "        self.layer3 = bnn.SquareDense(100, 1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.bihomogeneous(inputs)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = tf.math.log(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXSQuJgCtGDI"
      },
      "outputs": [],
      "source": [
        "model = Kahler_potential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vooMAXq2tGDJ"
      },
      "source": [
        "Define the Kähler metric $g_{i \\bar j} = \\partial_i\\bar\\partial_{\\bar j} K$ and the volume form $d\\mu_g = \\det g_{i \\bar j}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKNPTfxPtGDJ"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def volume_form(points, Omega_Omegabar, mass, restriction):\n",
        "\n",
        "    kahler_metric = mlg.complex_math.complex_hessian(tf.math.real(model(points)), points)\n",
        "    kahler_metric = tf.matmul(restriction, tf.matmul(kahler_metric, restriction, adjoint_b=True))\n",
        "    det_g = tf.math.real(tf.linalg.det(kahler_metric))\n",
        "\n",
        "    # Calculate the normalization constant to make the overall integration as 1\n",
        "    # It is a batchwise calculation but we expect it to converge to a constant eventually\n",
        "    # Consequently, if one computes the average of volume_form / Omega_Omegabar,\n",
        "    # they will get strictly 1. (Actually the result would be Vol_Omega, but we set\n",
        "    # it to be 1 here implicitly.)\n",
        "    weights = mass / tf.reduce_sum(mass)\n",
        "    factor = tf.reduce_sum(weights * det_g / Omega_Omegabar)\n",
        "    volume_form = det_g / factor\n",
        "\n",
        "    return volume_form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSvy2QCmtGDJ"
      },
      "source": [
        "### Train the model with Adam and L-BFGS\n",
        "#### Adam\n",
        "Setup the keras optmizer as `Adam` and the loss function as one of weighted loss in the `mlg.loss` module. Some available functions are `weighted_MAPE`, `weighted_MSE`, `max_error` and `MAPE_plus_max_error`. They are weighted with the mass formula since the points on the hypersurface are distributed according to the Fubini-Study measure while the measure used in the integration is determined by the volume form $\\small \\Omega \\wedge \\bar\\Omega$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jeIUXSTtGDK"
      },
      "outputs": [],
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "loss_func = mlg.loss.weighted_MAPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8R-_rO7tGDK"
      },
      "source": [
        "Loop over the batches and train the network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WNWbQo1LtGDK",
        "outputId": "208485bb-883e-4852-b673-6882889f1ede",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10: loss = 0.00413\n",
            "epoch 20: loss = 0.00361\n",
            "epoch 30: loss = 0.00258\n",
            "epoch 40: loss = 0.00293\n",
            "epoch 50: loss = 0.00274\n"
          ]
        }
      ],
      "source": [
        "max_epochs = 50\n",
        "epoch = 0\n",
        "while epoch < max_epochs:\n",
        "    epoch = epoch + 1\n",
        "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(train_set):\n",
        "        with tf.GradientTape() as tape:\n",
        "            det_omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
        "            loss = loss_func(Omega_Omegabar, det_omega, mass)\n",
        "            grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"epoch %d: loss = %.5f\" % (epoch, loss))\n",
        "        #print(\"%.5f\" % (loss))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAVB6kgztGDL"
      },
      "source": [
        "Let's check the loss of the test dataset. First define a function to calculate the total loss over the whole dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqGWnTq6tGDL"
      },
      "outputs": [],
      "source": [
        "def cal_total_loss(dataset, loss_function):\n",
        "    total_loss = tf.constant(0, dtype=tf.float32)\n",
        "    total_mass = tf.constant(0, dtype=tf.float32)\n",
        "\n",
        "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(dataset):\n",
        "        det_omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
        "        mass_sum = tf.reduce_sum(mass)\n",
        "        total_loss += loss_function(Omega_Omegabar, det_omega, mass) * mass_sum\n",
        "        total_mass += mass_sum\n",
        "    total_loss = total_loss / total_mass\n",
        "\n",
        "    return total_loss.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGmTMIytGDL"
      },
      "source": [
        "Check the results of MAPE and MSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7gZJSOdtGDL",
        "outputId": "1e27be04-2e00-41b4-b49b-1dead326d6b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigma_test = 0.00320\n",
            "E_test = 0.00002\n"
          ]
        }
      ],
      "source": [
        "sigma_test = cal_total_loss(test_set, mlg.loss.weighted_MAPE)\n",
        "E_test = cal_total_loss(test_set, mlg.loss.weighted_MSE)\n",
        "print(\"sigma_test = %.5f\" % sigma_test)\n",
        "print(\"E_test = %.5f\" % E_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omQZi7TytGDL"
      },
      "source": [
        "You can also check the error of the Monte Carlo integration, estimated by:\n",
        "\n",
        "$$\\delta \\sigma = \\frac{1}{\\sqrt{N_p}} {\\left( \\int_X (|\\eta - 1_X| - \\sigma)^2 d\\mu_{\\Omega}\\right)}^{1/2},$$\n",
        "\n",
        "where $N_p$ is the number of points on the hypersurface and $\\sigma$ is the `weighted_MAPE` loss, and\n",
        "\n",
        "$$\\eta = \\frac{\\det \\omega}{\\small \\Omega \\wedge \\bar \\Omega}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY2fn1iFtGDM",
        "outputId": "47d10995-9f2f-4442-bb3a-3eafba5e72a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta_simga = 0.00001\n"
          ]
        }
      ],
      "source": [
        "def delta_sigma_square_test(y_true, y_pred, mass):\n",
        "    weights = mass / tf.reduce_sum(mass)\n",
        "    return tf.reduce_sum((tf.abs(y_true - y_pred) / y_true - sigma_test)**2 * weights)\n",
        "\n",
        "delta_sigma = cal_total_loss(test_set, delta_sigma_square_test)\n",
        "print(\"delta_simga = %.5f\" % delta_sigma)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def volume_form_with_model(points, Omega_Omegabar, mass, restriction, this_model):\n",
        "\n",
        "    kahler_metric = mlg.complex_math.complex_hessian(tf.math.real(this_model(points)), points)\n",
        "    kahler_metric = tf.matmul(restriction, tf.matmul(kahler_metric, restriction, adjoint_b=True))\n",
        "    volume_form = tf.math.real(tf.linalg.det(kahler_metric))\n",
        "\n",
        "    # Calculate the normalization constant to make the overall integration as 1\n",
        "    # It is a batchwise calculation but we expect it to converge to a constant eventually\n",
        "    weights = mass / tf.reduce_sum(mass)\n",
        "    factor = tf.reduce_sum(weights * volume_form / Omega_Omegabar)\n",
        "\n",
        "    # Frank's modification\n",
        "    #print(tf.executing_eagerly())\n",
        "\n",
        "    \"\"\"\n",
        "    N   = tf.shape(kahler_metric)[0]\n",
        "    idx = tf.random.shuffle(tf.range(N))[:100]\n",
        "    M   = tf.gather(kahler_metric, idx)\n",
        "\n",
        "    for i in tf.range(tf.shape(M)[0]):\n",
        "        tf.print(\"matrix\", i, idx[i], \"\\n real:\\n\", tf.math.real(M[i]),\n",
        "                 \"\\n imag:\\n\", tf.math.imag(M[i]), summarize=-1)\n",
        "    \"\"\"\n",
        "\n",
        "    file_name = 'metric_ellptic_xt.csv'\n",
        "\n",
        "    with open(file_name,'w',newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        #writer.writerow(['Point Index','Points','Kahler Metric (Real)','Kahler Metric (Imag)'])\n",
        "        for i in range(120):\n",
        "            #tf.print(\"matrix\", i, pts[i], tf.math.real(kahler_metric[i]), tf.math.imag(kahler_metric[i]), summarize=-1)\n",
        "            #tf.print(i, \"points=\", pts[i], \"Kahler_metric_real=\", tf.math.real(kahler_metric[i]), \"Kahler_metric_imag=\", tf.math.imag(kahler_metric[i]), summarize=-1,output_stream=\"file://\"+file_name)\n",
        "            tf.print([i, \";\", pts[i], \";\", tf.math.real(kahler_metric[i]), \";\", tf.math.imag(kahler_metric[i])], summarize=-1, output_stream=\"file://\"+file_name)\n",
        "    print(f\"Done! Results in {file_name}\")\n",
        "\n",
        "    \"\"\"\n",
        "    for i in range(120):\n",
        "        tf.print(\"matrix\", i, pts[i], tf.math.real(kahler_metric[i]), tf.math.imag(kahler_metric[i]), summarize=-1)\n",
        "\n",
        "\n",
        "\n",
        "    # Convert 2x2 complex to 4x4 real Hermitian?\n",
        "    # See \"Riemannian metric.ipynb\"\n",
        "\n",
        "    #eigvals = tf.linalg.eigh(M)[0]\n",
        "    #eigvals_np = eigvals.numpy()\n",
        "    \"\"\"\n",
        "    return volume_form / factor"
      ],
      "metadata": {
        "id": "lLl2MXn05J0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Fubini_potential(tf.keras.Model):\n",
        "    def __init__(self, trainable=True, dtype='float32', **kwargs):\n",
        "        super(Fubini_potential, self).__init__(trainable=trainable, dtype=dtype, **kwargs)\n",
        "        self.abs_layer = tf.keras.layers.Lambda(lambda x: tf.abs(x))\n",
        "        self.layer1 = bnn.SquareDense(len(Z), len(Z), activation=tf.square)\n",
        "        self.layer2 = bnn.SquareDense(len(Z), 1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.abs_layer(inputs)  # Convert complex64 to float32 by taking the absolute value\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = tf.math.log(x)\n",
        "        return x\n",
        "\n",
        "fubi = Fubini_potential()\n",
        "fubi.layer1.w.assign(tf.constant([[1,0,0],[0,1,0],[0,0,1]], dtype=tf.float32))\n",
        "fubi.layer2.w.assign(tf.constant([[1],[1],[1]], dtype=tf.float32))\n",
        "\"\"\"\n",
        "fubi.layer1.w.assign(tf.constant([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]], dtype=tf.float32))\n",
        "fubi.layer2.w.assign(tf.constant([[1],[1],[1],[1]], dtype=tf.float32))\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "riPaV6Yl5mQt",
        "outputId": "db0f586f-e162-4608-873d-ce99065d76cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfubi.layer1.w.assign(tf.constant([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]], dtype=tf.float32))\\nfubi.layer2.w.assign(tf.constant([[1],[1],[1],[1]], dtype=tf.float32))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_quotient(y_numerator, y_denominator, mass):  # mass-weighted\n",
        "    weights = mass / tf.reduce_sum(mass)\n",
        "    return tf.reduce_sum(tf.abs(y_numerator / y_denominator) * weights)\n",
        "\n",
        "def calculate_volume(dataset):\n",
        "    total_holo_volume = tf.constant(0, dtype=tf.float32)\n",
        "    total_kaehler_volume = tf.constant(0, dtype=tf.float32)\n",
        "    total_fubi_volume = tf.constant(0, dtype=tf.float32)\n",
        "    total_mass = tf.constant(0, dtype=tf.float32)\n",
        "    num_points = 0\n",
        "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(dataset):\n",
        "        det_fubi = volume_form_with_model(points, Omega_Omegabar, mass, restriction, fubi)\n",
        "        det_omega = volume_form_with_model(points, Omega_Omegabar, mass, restriction, model)\n",
        "        mass_sum = tf.reduce_sum(mass)\n",
        "        total_holo_volume += compute_quotient(Omega_Omegabar, det_fubi, mass) * mass_sum\n",
        "        total_kaehler_volume += compute_quotient(det_omega, det_fubi, mass) * mass_sum\n",
        "        total_fubi_volume += compute_quotient(det_fubi, det_fubi, mass)\n",
        "        total_mass += mass_sum\n",
        "        num_points += len(points)\n",
        "    total_holo_volume = total_holo_volume / total_mass\n",
        "    total_kaehler_volume = total_kaehler_volume / total_mass\n",
        "\n",
        "    return num_points, total_mass.numpy(), total_holo_volume.numpy(), total_kaehler_volume.numpy(), total_fubi_volume.numpy()\n",
        "\n",
        "num_points, total_mass, holo_vol, kaehler_vol, fubi_volume = calculate_volume(test_set)\n",
        "print(f'Num points: {num_points}; Total mass: {total_mass}; Holomorphic volume: {holo_vol}; Kahler volume: {kaehler_vol}; Fubi volume: {fubi_volume}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhdIJZre5gFp",
        "outputId": "7f2fb003-8e67-401a-beaa-ff8b86717aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Results in metric_ellptic_xt.csv\n",
            "Done! Results in metric_ellptic_xt.csv\n",
            "Num points: 3840; Total mass: 17358.966796875; Holomorphic volume: 1.0306395292282104; Kahler volume: 1.0304555892944336; Fubi volume: 30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_quotient(y_numerator, y_denominator, mass): # batch-average\n",
        "    weights = mass / tf.reduce_sum(mass)\n",
        "    return tf.reduce_sum(tf.abs(y_numerator / y_denominator) * weights)\n",
        "\n",
        "def calculate_volume(dataset):\n",
        "    total_holo_volume = tf.constant(0, dtype=tf.float32)\n",
        "    total_kaehler_volume = tf.constant(0, dtype=tf.float32)\n",
        "    total_fubi_volume = tf.constant(0, dtype=tf.float32)\n",
        "    total_mass = tf.constant(0, dtype=tf.float32)\n",
        "    num_points = 0\n",
        "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(dataset):\n",
        "        det_fubi = volume_form_with_model(points, Omega_Omegabar, mass, restriction, fubi)\n",
        "        det_omega = volume_form_with_model(points, Omega_Omegabar, mass, restriction, model)\n",
        "        mass_sum = tf.reduce_sum(mass)\n",
        "        total_holo_volume += compute_quotient(Omega_Omegabar, Omega_Omegabar, mass)\n",
        "        total_kaehler_volume += compute_quotient(det_omega, Omega_Omegabar, mass)\n",
        "        total_fubi_volume += compute_quotient(det_fubi, Omega_Omegabar, mass)\n",
        "        total_mass += mass_sum\n",
        "        num_points += len(points)\n",
        "    total_holo_volume = total_holo_volume / total_mass\n",
        "    total_kaehler_volume = total_kaehler_volume / total_mass\n",
        "\n",
        "    return num_points, total_mass.numpy(), total_holo_volume.numpy(), total_kaehler_volume.numpy(), total_fubi_volume.numpy()\n",
        "\n",
        "num_points, total_mass, holo_vol, kaehler_vol, fubi_volume = calculate_volume(test_set)\n",
        "print(f'Num points: {num_points}; Total mass: {total_mass}; Holomorphic volume: {holo_vol}; Kahler volume: {kaehler_vol}; Fubi volume: {fubi_volume}')"
      ],
      "metadata": {
        "id": "Yam1-qw3txU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad57e29d-da65-4f7b-e3d0-06ff5f87d9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num points: 3840; Total mass: 17358.96875; Holomorphic volume: 0.001728213275782764; Kahler volume: 0.001728213275782764; Fubi volume: 30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('metric_ellptic_xt.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "R6tIdhC9eg3I",
        "outputId": "5e07cb6e-2034-40d4-85c9-0ef5e13ebf78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_30a364b9-e26e-4383-8e1c-eae8b69a4b06\", \"metric_ellptic_xt.csv\", 2435373)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (IRRELEVANT)\n",
        "Let's see if the computation matches our manual computation of the ratio, at point q = (1, exp(i pi/4), 0, 0),\n",
        "\n",
        "The manually calculated ratio is 1/16 (or i/16 up to a factor of i/2pi)"
      ],
      "metadata": {
        "id": "EPKhszPl96Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def ratio_in_batch(points, Omega_Omegabar, mass, restriction):\n",
        "    \"\"\"\n",
        "    Returns a Tensor of shape (batch_size,) with\n",
        "    ratio[i] = Omega_Omegabar[i] / det(model's metric)[i].\n",
        "    This uses the raw determinant before any normalization is applied.\n",
        "    \"\"\"\n",
        "    kahler_metric = mlg.complex_math.complex_hessian(tf.math.real(model(points)), points)\n",
        "    kahler_metric = tf.matmul(\n",
        "        restriction,\n",
        "        tf.matmul(kahler_metric, restriction, adjoint_b=True)\n",
        "    )\n",
        "\n",
        "    det_kahler = tf.math.real(tf.linalg.det(kahler_metric))\n",
        "\n",
        "    ratio = Omega_Omegabar / det_kahler\n",
        "\n",
        "    return ratio"
      ],
      "metadata": {
        "id": "kAaYceWo7Ee_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ratios = []\n",
        "for step, (points, Omega_Omegabar, mass, restriction) in enumerate(test_set):\n",
        "    batch_ratio = ratio_in_batch(points, Omega_Omegabar, mass, restriction)\n",
        "    all_ratios.append(batch_ratio)\n",
        "\n",
        "all_ratios = tf.concat(all_ratios, axis=0)\n",
        "\n",
        "all_ratios_np = all_ratios.numpy()\n",
        "\n",
        "print(\"Ratio array shape:\", all_ratios_np.shape)"
      ],
      "metadata": {
        "id": "a9lRZMlN8uKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eff10ff-b201-4210-9c11-90f606799e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratio array shape: (3840,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_np = np.array([1.0, 0.0, 0.0], dtype=np.complex64)\n",
        "\n",
        "THRESHOLD = 0.45\n",
        "all_close_ratios = []\n",
        "\n",
        "for step, (points, Omega_Omegabar, mass, restriction) in enumerate(test_set):\n",
        "    points_np = points.numpy()  # shape (batch_size, 4) if CP^3\n",
        "\n",
        "    dist = np.linalg.norm(points_np - q_np, axis=1)\n",
        "\n",
        "    mask = dist < THRESHOLD\n",
        "\n",
        "    if not np.any(mask):\n",
        "        continue\n",
        "\n",
        "    batch_ratios = ratio_in_batch(points, Omega_Omegabar, mass, restriction)\n",
        "    batch_ratios_np = batch_ratios.numpy()\n",
        "\n",
        "    close_ratios = batch_ratios_np[mask]\n",
        "    all_close_ratios.append(close_ratios)\n",
        "\n",
        "if len(all_close_ratios) == 0:\n",
        "    print(\"No points found within distance\", THRESHOLD)\n",
        "else:\n",
        "    all_close_ratios = np.concatenate(all_close_ratios, axis=0)\n",
        "    print(f\"Found {len(all_close_ratios)} points within distance {THRESHOLD}.\")\n",
        "    print(\"Mean ratio near q:\", all_close_ratios.mean())\n",
        "    print(\"Std dev ratio near q:\", all_close_ratios.std())"
      ],
      "metadata": {
        "id": "vs3iQc8a9W6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746978c3-1c55-420c-f2f0-fe13309e2183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 86 points within distance 0.45.\n",
            "Mean ratio near q: 0.56973535\n",
            "Std dev ratio near q: 0.0015814168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_CrpS8ZM_RBA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ilHaPYnkEi-S"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}