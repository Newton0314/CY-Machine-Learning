{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FIm6mUYtGC5"
      },
      "source": [
        "# MLGeometry guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCemOECptGC6"
      },
      "source": [
        "This introduction demonstrates how to use MLGeometry to:\n",
        "1. Generate a hypersurface.\n",
        "2. Build a bihomogeneous neural network.\n",
        "3. Use the model to compute numerical Calabi-Yau metrics with the embedding method.\n",
        "4. Plot $\\eta$ on a rational curve."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the package (on Colab)"
      ],
      "metadata": {
        "id": "ilHaPYnkEi-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install MLGeometry-tf"
      ],
      "metadata": {
        "id": "5pVEmL9vErvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab55b785-654f-4c22-d889-cca942c6e409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: MLGeometry-tf in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: tensorflow-probability[tf] in /usr/local/lib/python3.12/dist-packages (from MLGeometry-tf) (0.25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from MLGeometry-tf) (1.14.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from MLGeometry-tf) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->MLGeometry-tf) (2.9.0.post0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->MLGeometry-tf) (1.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (1.17.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (5.2.1)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (3.1.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (0.1.9)\n",
            "Requirement already satisfied: tensorflow>=2.16 in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (2.19.1)\n",
            "Requirement already satisfied: tf-keras>=2.16 in /usr/local/lib/python3.12/dist-packages (from tensorflow-probability[tf]->MLGeometry-tf) (2.19.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (25.2.10)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.5.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree->tensorflow-probability[tf]->MLGeometry-tf) (25.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (14.1.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.16->tensorflow-probability[tf]->MLGeometry-tf) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyFvWKNmtGC7"
      },
      "source": [
        "## Configure imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhqc2oMWtGC8"
      },
      "source": [
        "Import tensorflow_probability to use the L-BFGS optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doBhWopntGC9"
      },
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import keras\n",
        "import csv\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJJvGaCNtGC-"
      },
      "outputs": [],
      "source": [
        "import MLGeometry as mlg\n",
        "from MLGeometry import bihomoNN as bnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9F3AKqPtGC_"
      },
      "source": [
        "Import the libraries to plot the $\\eta$ on the rational curve (see the last section):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9LWSTH3tGC_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd1zG07TtGDA"
      },
      "source": [
        "## Set a random seed (optional)\n",
        "Some random seed might be bad for numerical calulations. If there are any errors during the training, you may want to try a different seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5Rz0lXmtGDB"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjB84Ln1tGDB"
      },
      "source": [
        "## Define a hypersurface\n",
        "First define a set of coordinates and a function as sympy symbols:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwhQynDBtGDB"
      },
      "outputs": [],
      "source": [
        "# Dwork family (indexed by t) of the Fermat quartic in CP^3\n",
        "z0, z1, z2, z3 = sp.symbols('z0, z1, z2, z3')\n",
        "Z = [z0, z1, z2, z3]\n",
        "\n",
        "fermat = z0**4 + z1**4 + z2**4 + z3**4\n",
        "\n",
        "zeta5 = np.exp(2*np.pi*1j/5)\n",
        "\n",
        "t = 0.0001\n",
        "\n",
        "# Dwork family (indexed by t) in CP^3\n",
        "f_x = fermat + t*z0*z1*z2*z3  # X_t\n",
        "#f_z = t*fermat + (z1**4 + z2**4 + z3**4) + z0*(z1**3 + z2**3 + z3**3) + z0**3 * (z1 + z2 + z3)  # Z_t (wrong equation)\n",
        "f_z = t*fermat + (z1**4 + z2**4 + z3**4) + z0*(z1**3 + z2**3 + z3**3) + z0**2 * (z1**2 + z2**2 + z3**2)  # Z_t\n",
        "#f_w = t*fermat + (z0**2 + z1*z2 + z3**2)*(z1**2 + z2*z3 + z0**2)  # W_t, small\n",
        "f_w = t*fermat + z0*(z0**3 + z1**3 + z2**3 + z3**3) # W_t\n",
        "f_v = t*fermat + z0*z1*z2*z3  # V_t, large\n",
        "f_w2 = t*fermat + z0*(z1**3 + z2**3 + z3**3)\n",
        "\n",
        "f = f_w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTm215kftGDC"
      },
      "source": [
        "Then define a hypersurface as a collection of points which solve the equation f = 0, using the `Hypersurface` class in the `mlg.hypersurface` module. The parameter n_pairs is the number of random pairs of points used to form the random lines in $\\mathbf{CP}^{N+1}$. Then we take the intersections of those random lines and the hypersurface. By Bezout's theorem, each line intersects the hypersurface in precisely d points where d is the number of homogeneous coordinates. So the total number of points is d * n_pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz1Vi4Y2tGDC"
      },
      "outputs": [],
      "source": [
        "n_pairs = 10240\n",
        "HS_train = mlg.hypersurface.Hypersurface(Z, f, n_pairs)\n",
        "HS_test = mlg.hypersurface.Hypersurface(Z, f, n_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE981r2ctGDC"
      },
      "source": [
        "The Hypersurface class will take care of the patchwork automatically. Let's use the `list_patches` function to check the number of points on each patch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zyw84dftGDC",
        "outputId": "62cdaf8e-9a03-482e-8bab-03420081c936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Patches: 4\n",
            "Points on patch 1 : 7622\n",
            "Points on patch 2 : 11325\n",
            "Points on patch 3 : 11095\n",
            "Points on patch 4 : 10918\n"
          ]
        }
      ],
      "source": [
        "HS_train.list_patches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JtfwpmFtGDD"
      },
      "source": [
        "You can also invoke this method on one of the patches to check the distribution on the subpatches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fMZAyKutGDD",
        "outputId": "9a89fa06-4f45-407b-f4cc-5373ef76f0c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Patches: 3\n",
            "Points on patch 1 : 2541\n",
            "Points on patch 2 : 2548\n",
            "Points on patch 3 : 2533\n"
          ]
        }
      ],
      "source": [
        "HS_train.patches[0].list_patches()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8p5D9ZWtGDE"
      },
      "source": [
        "The Hypersurface class contains some symbolic and numerical methods as well, which will be introduced elsewhere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u1oIOLytGDE"
      },
      "source": [
        "## Training with Tensorflow\n",
        "The following steps are similar to a regular Tensorflow training process.\n",
        "### Generate datasets\n",
        "The `mlg.tf_dataset.generate_dataset` function converts a hypersurface to a Tensorflow Dataset, which has four componets: the points on the hypersurface, the volume form $\\small \\Omega \\wedge \\bar\\Omega$, the mass reweighting the points distribution and the restriction which restricts the Kähler metric to a subpatch. The restriction contains an extra linear transformation so that points on different affine patches can all be processed in one call. It is also possible to generate a dataset only on one affine patch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGq-mKdDtGDE"
      },
      "outputs": [],
      "source": [
        "train_set = mlg.tf_dataset.generate_dataset(HS_train)\n",
        "test_set = mlg.tf_dataset.generate_dataset(HS_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHP3ExA1tGDG"
      },
      "source": [
        "Shuffle and batch the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1AX_cm_tGDG"
      },
      "outputs": [],
      "source": [
        "train_set = train_set.shuffle(HS_train.n_points).batch(1024)\n",
        "test_set = test_set.shuffle(HS_test.n_points).batch(1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7Mga5fdtGDG"
      },
      "source": [
        "Let's look at what is inside a dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kwHPcIFHtGDH",
        "outputId": "a5d8e736-56ce-435d-c121-66dccd07ded7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[-8.7093045e-05+3.107390e-05j -2.2376217e-02-7.512015e-02j\n",
            "  1.0000000e+00-0.000000e+00j -1.6000953e-01-6.569266e-01j], shape=(4,), dtype=complex64)\n"
          ]
        }
      ],
      "source": [
        "points, Omega_Omegabar, mass, restriction = next(iter(train_set))\n",
        "print(points[1])\n",
        "pts = points.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kVSGnnktGDH"
      },
      "source": [
        "### Build a bihomogeneous neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV_JRrERtGDI"
      },
      "source": [
        "The `mlg.bihomoNN` module provides the necessary layers (e.g. `Bihomogeneous` and `Dense` ) to construct the Kähler potential with a bihomogeneous neural network. Here is an example of a two-hidden-layer network (k = 4) with 70 and 100 hidden units:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9mE_YLltGDI"
      },
      "outputs": [],
      "source": [
        "@keras.saving.register_keras_serializable(package=\"MLGeometry\")\n",
        "class Kahler_potential(tf.keras.Model):\n",
        "    def __init__(self, trainable=True, dtype='float32', **kwargs):\n",
        "        super(Kahler_potential, self).__init__(trainable=trainable, dtype=dtype, **kwargs)\n",
        "        # The first layer transforms the complex points to the bihomogeneous form.\n",
        "        # The number of the outputs is d^2, where d is the number of coordinates.\n",
        "        self.bihomogeneous = bnn.Bihomogeneous(d=len(Z))\n",
        "        self.layer1 = bnn.SquareDense(len(Z)**2, 70, activation=tf.square)\n",
        "        self.layer2 = bnn.SquareDense(70, 100, activation=tf.square)\n",
        "        self.layer3 = bnn.SquareDense(100, 1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.bihomogeneous(inputs)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = tf.math.log(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXSQuJgCtGDI"
      },
      "outputs": [],
      "source": [
        "model = Kahler_potential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vooMAXq2tGDJ"
      },
      "source": [
        "Define the Kähler metric $g_{i \\bar j} = \\partial_i\\bar\\partial_{\\bar j} K$ and the volume form $d\\mu_g = \\det g_{i \\bar j}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKNPTfxPtGDJ"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def volume_form(points, Omega_Omegabar, mass, restriction):\n",
        "\n",
        "    kahler_metric = mlg.complex_math.complex_hessian(tf.math.real(model(points)), points)\n",
        "    kahler_metric = tf.matmul(restriction, tf.matmul(kahler_metric, restriction, adjoint_b=True))\n",
        "    det_g = tf.math.real(tf.linalg.det(kahler_metric))\n",
        "\n",
        "    # Calculate the normalization constant to make the overall integration as 1\n",
        "    # It is a batchwise calculation but we expect it to converge to a constant eventually\n",
        "    # Consequently, if one computes the average of volume_form / Omega_Omegabar,\n",
        "    # they will get strictly 1. (Actually the result would be Vol_Omega, but we set\n",
        "    # it to be 1 here implicitly.)\n",
        "    weights = mass / tf.reduce_sum(mass)\n",
        "    factor = tf.reduce_sum(weights * det_g / Omega_Omegabar)\n",
        "    volume_form = det_g / factor\n",
        "\n",
        "    return volume_form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSvy2QCmtGDJ"
      },
      "source": [
        "### Train the model with Adam and L-BFGS\n",
        "#### Adam\n",
        "Setup the keras optmizer as `Adam` and the loss function as one of weighted loss in the `mlg.loss` module. Some available functions are `weighted_MAPE`, `weighted_MSE`, `max_error` and `MAPE_plus_max_error`. They are weighted with the mass formula since the points on the hypersurface are distributed according to the Fubini-Study measure while the measure used in the integration is determined by the volume form $\\small \\Omega \\wedge \\bar\\Omega$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jeIUXSTtGDK"
      },
      "outputs": [],
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "loss_func = mlg.loss.weighted_MAPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8R-_rO7tGDK"
      },
      "source": [
        "Loop over the batches and train the network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WNWbQo1LtGDK",
        "outputId": "93f7db76-7b8c-4834-9799-68eb6bafc098",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.73387\n",
            "1.00683\n",
            "1.07998\n",
            "1.52539\n",
            "1.14131\n",
            "1.14124\n",
            "1.16132\n",
            "0.98491\n",
            "1.57936\n",
            "1.04317\n",
            "1.04090\n",
            "1.08336\n",
            "0.73380\n",
            "1.28546\n",
            "1.05272\n",
            "1.06281\n",
            "1.37894\n",
            "0.86883\n",
            "1.30213\n",
            "0.95722\n",
            "0.97591\n",
            "1.18970\n",
            "1.13539\n",
            "0.80994\n",
            "1.16226\n",
            "1.17470\n",
            "1.53587\n",
            "1.18533\n",
            "0.93428\n",
            "1.24901\n",
            "1.38406\n",
            "1.29032\n",
            "1.19226\n",
            "1.10649\n",
            "1.00736\n",
            "1.37810\n",
            "0.98869\n",
            "0.94108\n",
            "1.24631\n",
            "0.95695\n",
            "1.00767\n",
            "1.21602\n",
            "1.03528\n",
            "0.95283\n",
            "1.04464\n",
            "0.91464\n",
            "0.84666\n",
            "0.99798\n",
            "1.53440\n",
            "1.04940\n",
            "1.26334\n",
            "1.28934\n",
            "1.17048\n",
            "0.93123\n",
            "0.87832\n",
            "0.87725\n",
            "1.20005\n",
            "1.23908\n",
            "0.94832\n",
            "1.18580\n",
            "0.78499\n",
            "1.12391\n",
            "1.19919\n",
            "1.04764\n",
            "0.79017\n",
            "0.96115\n",
            "1.05839\n",
            "1.41646\n",
            "1.28389\n",
            "1.44951\n",
            "1.32758\n",
            "0.73174\n",
            "1.50632\n",
            "1.14382\n",
            "0.99979\n",
            "0.91503\n",
            "1.00625\n",
            "0.98042\n",
            "0.96963\n",
            "0.87072\n",
            "0.89489\n",
            "0.99851\n",
            "0.69696\n",
            "1.37270\n",
            "0.88549\n",
            "0.71391\n",
            "0.97004\n",
            "1.06602\n",
            "0.74351\n",
            "0.62750\n",
            "0.91454\n",
            "0.81905\n",
            "0.89969\n",
            "0.84003\n",
            "0.82000\n",
            "1.19618\n",
            "0.88748\n",
            "0.79795\n",
            "0.91657\n",
            "0.86825\n",
            "1.15356\n",
            "0.93580\n",
            "1.01658\n",
            "0.57056\n",
            "0.75072\n",
            "0.71260\n",
            "0.79517\n",
            "0.90223\n",
            "0.65850\n",
            "1.01599\n",
            "0.80996\n",
            "1.26746\n",
            "0.72283\n",
            "0.69042\n",
            "0.61976\n",
            "0.79974\n",
            "0.86619\n",
            "0.62237\n",
            "1.28882\n",
            "0.77171\n",
            "0.63036\n",
            "1.09452\n",
            "0.66953\n",
            "0.75583\n",
            "0.93818\n",
            "0.70012\n",
            "1.15038\n",
            "1.22235\n",
            "1.09537\n",
            "0.74860\n",
            "1.29442\n",
            "0.84339\n",
            "0.64334\n",
            "1.20583\n",
            "1.30654\n",
            "0.98272\n",
            "0.71112\n",
            "1.34923\n",
            "0.64041\n",
            "0.64897\n",
            "0.64490\n",
            "0.41797\n",
            "0.65024\n",
            "0.68115\n",
            "0.65329\n",
            "0.60502\n",
            "0.95686\n",
            "0.58230\n",
            "0.65219\n",
            "1.16400\n",
            "0.65433\n",
            "0.45064\n",
            "1.08486\n",
            "0.67203\n",
            "0.70230\n",
            "0.63737\n",
            "0.50475\n",
            "0.62108\n",
            "0.45515\n",
            "0.74191\n",
            "0.51291\n",
            "0.60953\n",
            "0.65157\n",
            "0.98058\n",
            "0.85967\n",
            "0.56103\n",
            "0.38297\n",
            "0.67495\n",
            "0.58862\n",
            "0.51683\n",
            "1.00940\n",
            "0.94222\n",
            "0.55715\n",
            "0.95858\n",
            "0.70841\n",
            "1.17516\n",
            "0.48988\n",
            "0.85518\n",
            "0.53584\n",
            "0.69949\n",
            "0.96255\n",
            "0.71633\n",
            "0.62931\n",
            "0.53860\n",
            "0.53906\n",
            "0.86063\n",
            "0.49514\n",
            "0.96147\n",
            "0.50252\n",
            "0.31562\n",
            "0.73315\n",
            "0.37533\n",
            "0.49642\n",
            "0.74868\n",
            "0.80825\n",
            "0.55671\n",
            "1.08430\n",
            "0.63351\n",
            "0.92956\n",
            "1.30191\n",
            "0.48040\n",
            "1.13150\n",
            "0.97726\n",
            "1.04859\n",
            "0.70713\n",
            "0.35030\n",
            "0.79587\n",
            "0.42390\n",
            "0.67086\n",
            "0.50337\n",
            "0.47817\n",
            "0.41611\n",
            "0.61612\n",
            "0.37723\n",
            "0.15730\n",
            "0.63302\n",
            "0.38481\n",
            "0.57059\n",
            "0.73665\n",
            "0.40227\n",
            "0.52861\n",
            "0.44249\n",
            "0.53825\n",
            "0.92077\n",
            "0.55085\n",
            "0.45117\n",
            "0.92165\n",
            "0.52177\n",
            "0.64540\n",
            "0.68101\n",
            "0.54924\n",
            "0.65030\n",
            "0.62771\n",
            "0.56299\n",
            "0.52538\n",
            "0.62441\n",
            "0.52646\n",
            "0.33162\n",
            "0.58427\n",
            "0.43008\n",
            "0.72836\n",
            "0.52568\n",
            "1.03174\n",
            "0.64963\n",
            "0.83332\n",
            "0.85982\n",
            "0.56448\n",
            "0.89262\n",
            "1.06374\n",
            "0.50857\n",
            "0.54310\n",
            "0.63408\n",
            "0.91927\n",
            "0.90052\n",
            "0.53690\n",
            "1.15779\n",
            "0.49227\n",
            "0.92666\n",
            "0.57607\n",
            "0.61260\n",
            "0.62007\n",
            "0.73356\n",
            "0.65709\n",
            "1.15579\n",
            "0.61009\n",
            "0.74829\n",
            "0.74607\n",
            "0.62101\n",
            "0.46771\n",
            "0.31733\n",
            "0.74303\n",
            "0.42765\n",
            "0.87615\n",
            "0.50805\n",
            "0.98038\n",
            "0.91657\n",
            "0.47831\n",
            "0.49736\n",
            "1.15958\n",
            "0.73827\n",
            "0.75377\n",
            "0.72377\n",
            "0.31768\n",
            "0.76433\n",
            "0.49248\n",
            "0.37027\n",
            "0.51376\n",
            "0.37078\n",
            "0.75659\n",
            "0.31744\n",
            "0.66845\n",
            "0.89831\n",
            "0.51204\n",
            "1.82821\n",
            "0.67909\n",
            "1.01628\n",
            "0.55852\n",
            "0.65120\n",
            "0.62036\n",
            "0.51645\n",
            "0.58262\n",
            "1.00627\n",
            "0.59247\n",
            "0.59007\n",
            "0.94610\n",
            "0.54731\n",
            "0.94832\n",
            "0.71883\n",
            "0.45057\n",
            "0.70829\n",
            "0.62281\n",
            "0.91606\n",
            "0.61138\n",
            "0.54769\n",
            "0.59433\n",
            "0.90100\n",
            "0.92653\n",
            "0.57617\n",
            "0.69149\n",
            "0.57935\n",
            "1.01476\n",
            "0.92797\n",
            "0.89308\n",
            "0.51050\n",
            "0.74224\n",
            "0.82035\n",
            "0.68437\n",
            "0.71880\n",
            "0.46158\n",
            "0.38942\n",
            "0.57679\n",
            "0.51962\n",
            "0.46600\n",
            "0.57462\n",
            "0.70069\n",
            "0.70225\n",
            "0.93160\n",
            "0.57544\n",
            "0.45293\n",
            "0.70209\n",
            "0.46672\n",
            "0.91778\n",
            "0.52407\n",
            "0.61719\n",
            "0.59528\n",
            "0.83222\n",
            "1.03192\n",
            "0.84190\n",
            "0.53113\n",
            "0.44211\n",
            "0.46484\n",
            "0.63554\n",
            "1.08393\n",
            "0.71675\n",
            "0.82154\n",
            "0.59845\n",
            "0.56256\n",
            "0.88834\n",
            "0.50935\n",
            "0.52874\n",
            "0.49889\n",
            "0.70413\n",
            "0.61051\n",
            "0.61173\n",
            "0.87651\n",
            "1.20173\n",
            "0.27110\n",
            "0.48133\n",
            "0.63929\n",
            "0.55378\n",
            "0.34338\n",
            "0.96828\n",
            "0.95714\n",
            "0.88522\n",
            "0.46701\n",
            "0.59210\n",
            "0.69725\n",
            "0.52203\n",
            "0.78217\n",
            "0.49518\n",
            "1.02120\n",
            "0.78794\n",
            "0.31840\n",
            "1.24580\n",
            "0.58733\n",
            "0.58600\n",
            "0.37400\n",
            "0.32260\n",
            "0.29897\n",
            "0.47833\n",
            "0.98808\n",
            "0.81185\n",
            "0.59680\n",
            "0.57309\n",
            "0.57839\n",
            "0.57806\n",
            "0.52669\n",
            "0.54084\n",
            "0.83317\n",
            "0.73505\n",
            "0.50324\n",
            "0.40332\n",
            "0.43303\n",
            "1.98866\n",
            "3.11759\n",
            "2.15292\n",
            "1.95703\n",
            "2.00160\n",
            "1.99334\n",
            "4.70296\n",
            "2.49875\n",
            "1.99455\n",
            "1.95697\n",
            "2.52099\n",
            "1.93565\n",
            "2.74768\n",
            "4.61920\n",
            "2.32021\n",
            "4.64478\n",
            "9.60872\n",
            "2.03697\n",
            "3.02461\n",
            "1.99197\n",
            "2.06989\n",
            "2.36537\n",
            "3.83320\n",
            "4.46000\n",
            "2.24404\n",
            "4.21119\n",
            "1.99453\n",
            "2.89885\n",
            "2.22307\n",
            "3.04550\n",
            "1.98507\n",
            "2.20749\n",
            "2.27222\n",
            "2.02160\n",
            "2.03494\n",
            "1.99781\n",
            "1.98955\n",
            "2.69925\n",
            "1.99536\n",
            "2.07810\n",
            "2.10745\n",
            "2.28006\n",
            "1.97790\n",
            "6.64211\n",
            "1.98201\n",
            "2.89680\n",
            "2.02184\n",
            "2.03098\n",
            "1.99762\n",
            "1.98840\n",
            "6.55933\n",
            "2.00619\n",
            "2.01520\n",
            "2.10660\n",
            "2.23277\n",
            "4.97653\n",
            "4.52766\n",
            "2.00788\n",
            "3.04270\n",
            "1.99982\n",
            "1.98425\n",
            "5.34584\n",
            "1.99654\n",
            "1.99592\n",
            "2.02641\n",
            "2.05045\n",
            "7.02726\n",
            "1.99325\n",
            "2.00571\n",
            "2.21751\n",
            "1.92064\n",
            "1.99399\n",
            "1.99503\n",
            "1.93668\n",
            "6.00540\n",
            "1.98149\n",
            "2.05937\n",
            "4.79089\n",
            "1.99256\n",
            "2.08188\n",
            "2.44760\n",
            "6.21223\n",
            "1.98174\n",
            "1.99472\n",
            "2.23378\n",
            "4.25386\n",
            "2.00402\n",
            "7.23504\n",
            "1.99347\n",
            "1.99760\n",
            "1.99675\n",
            "3.39222\n",
            "2.04829\n",
            "1.98641\n",
            "1.97875\n",
            "2.01707\n",
            "3.68806\n",
            "4.17240\n",
            "1.96649\n",
            "1.99112\n",
            "3.84960\n",
            "2.08790\n",
            "2.73533\n",
            "2.30595\n",
            "2.43117\n",
            "2.09873\n",
            "1.99130\n",
            "1.99082\n",
            "1.99820\n",
            "1.99685\n",
            "2.26136\n",
            "4.56258\n",
            "4.19774\n",
            "5.68700\n",
            "2.82650\n",
            "2.17936\n",
            "4.00119\n",
            "2.24620\n",
            "1.99555\n",
            "2.02584\n",
            "2.09445\n",
            "1.94980\n",
            "2.26040\n",
            "2.12468\n",
            "2.00641\n",
            "2.15810\n",
            "2.94373\n",
            "2.18716\n",
            "2.04087\n",
            "1.99096\n",
            "1.99460\n",
            "1.99993\n",
            "2.25179\n",
            "2.48337\n",
            "3.81108\n",
            "2.15259\n",
            "3.03113\n",
            "2.27008\n",
            "3.09919\n",
            "2.06868\n",
            "2.62337\n",
            "7.80761\n",
            "2.73587\n",
            "3.17690\n",
            "4.84473\n",
            "2.21611\n",
            "1.99745\n",
            "2.32076\n",
            "2.00635\n",
            "2.12451\n",
            "1.99090\n",
            "13.41073\n",
            "1.99355\n",
            "1.99532\n",
            "2.00212\n",
            "1.99579\n",
            "2.04632\n",
            "1.97263\n",
            "2.00458\n",
            "2.00656\n",
            "2.41937\n",
            "3.11787\n",
            "2.42191\n",
            "1.99440\n",
            "2.00013\n",
            "2.06133\n",
            "2.00056\n",
            "2.62585\n",
            "1.97761\n",
            "1.99931\n",
            "1.99010\n",
            "2.01706\n",
            "2.13730\n",
            "25.71627\n",
            "2.00865\n",
            "2.76581\n",
            "2.17641\n",
            "2.01179\n",
            "1.95586\n",
            "2.02580\n",
            "1.99994\n",
            "1.98689\n",
            "2.02391\n",
            "1.98690\n",
            "6.90845\n",
            "6.50277\n",
            "2.16659\n",
            "3.73713\n",
            "5.19444\n",
            "2.10902\n",
            "2.09672\n",
            "6.17712\n",
            "1.99270\n",
            "1.99829\n",
            "1.99952\n",
            "74.59937\n",
            "2.02140\n"
          ]
        }
      ],
      "source": [
        "max_epochs = 600\n",
        "epoch = 0\n",
        "while epoch < max_epochs:\n",
        "    epoch = epoch + 1\n",
        "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(train_set):\n",
        "        with tf.GradientTape() as tape:\n",
        "            det_omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
        "            loss = loss_func(Omega_Omegabar, det_omega, mass)\n",
        "            grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    if epoch % 1 == 0:\n",
        "        #print(\"epoch %d: loss = %.5f\" % (epoch, loss))\n",
        "        print(\"%.5f\" % (loss))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAVB6kgztGDL"
      },
      "source": [
        "Let's check the loss of the test dataset. First define a function to calculate the total loss over the whole dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqGWnTq6tGDL"
      },
      "outputs": [],
      "source": [
        "def cal_total_loss(dataset, loss_function):\n",
        "    total_loss = tf.constant(0, dtype=tf.float32)\n",
        "    total_mass = tf.constant(0, dtype=tf.float32)\n",
        "\n",
        "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(dataset):\n",
        "        det_omega = volume_form(points, Omega_Omegabar, mass, restriction)\n",
        "        mass_sum = tf.reduce_sum(mass)\n",
        "        total_loss += loss_function(Omega_Omegabar, det_omega, mass) * mass_sum\n",
        "        total_mass += mass_sum\n",
        "    total_loss = total_loss / total_mass\n",
        "\n",
        "    return total_loss.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGmTMIytGDL"
      },
      "source": [
        "Check the results of MAPE and MSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7gZJSOdtGDL",
        "outputId": "6e19ced2-67c7-49bc-dc47-6b178a97f5bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigma_test = 3.78873\n",
            "E_test = 3937.21460\n"
          ]
        }
      ],
      "source": [
        "sigma_test = cal_total_loss(test_set, mlg.loss.weighted_MAPE)\n",
        "E_test = cal_total_loss(test_set, mlg.loss.weighted_MSE)\n",
        "print(\"sigma_test = %.5f\" % sigma_test)\n",
        "print(\"E_test = %.5f\" % E_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omQZi7TytGDL"
      },
      "source": [
        "You can also check the error of the Monte Carlo integration, estimated by:\n",
        "\n",
        "$$\\delta \\sigma = \\frac{1}{\\sqrt{N_p}} {\\left( \\int_X (|\\eta - 1_X| - \\sigma)^2 d\\mu_{\\Omega}\\right)}^{1/2},$$\n",
        "\n",
        "where $N_p$ is the number of points on the hypersurface and $\\sigma$ is the `weighted_MAPE` loss, and\n",
        "\n",
        "$$\\eta = \\frac{\\det \\omega}{\\small \\Omega \\wedge \\bar \\Omega}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZY2fn1iFtGDM",
        "outputId": "1f7182cf-6db5-4a8b-f7a6-b0d36ead5dc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta_simga = 4481.43896\n"
          ]
        }
      ],
      "source": [
        "def delta_sigma_square_test(y_true, y_pred, mass):\n",
        "    weights = mass / tf.reduce_sum(mass)\n",
        "    return tf.reduce_sum((tf.abs(y_true - y_pred) / y_true - sigma_test)**2 * weights)\n",
        "\n",
        "delta_sigma = cal_total_loss(test_set, delta_sigma_square_test)\n",
        "print(\"delta_simga = %.5f\" % delta_sigma)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def volume_form_with_model(points, Omega_Omegabar, mass, restriction, this_model):\n",
        "\n",
        "    kahler_metric = mlg.complex_math.complex_hessian(tf.math.real(this_model(points)), points)\n",
        "    kahler_metric = tf.matmul(restriction, tf.matmul(kahler_metric, restriction, adjoint_b=True))\n",
        "    volume_form = tf.math.real(tf.linalg.det(kahler_metric))\n",
        "\n",
        "    # Calculate the normalization constant to make the overall integration as 1\n",
        "    # It is a batchwise calculation but we expect it to converge to a constant eventually\n",
        "    weights = mass / tf.reduce_sum(mass)\n",
        "    factor = tf.reduce_sum(weights * volume_form / Omega_Omegabar)\n",
        "\n",
        "    # Frank's modification\n",
        "\n",
        "    file_name = 'metric_k3_wt.csv'\n",
        "\n",
        "    with open(file_name,'w',newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        #writer.writerow(['Point Index','Points','Kahler Metric (Real)','Kahler Metric (Imag)'])\n",
        "        for i in range(120):\n",
        "            #tf.print(\"matrix\", i, pts[i], tf.math.real(kahler_metric[i]), tf.math.imag(kahler_metric[i]), summarize=-1)\n",
        "            #tf.print(i, \"points=\", pts[i], \"Kahler_metric_real=\", tf.math.real(kahler_metric[i]), \"Kahler_metric_imag=\", tf.math.imag(kahler_metric[i]), summarize=-1,output_stream=\"file://\"+file_name)\n",
        "            tf.print([i, \";\", pts[i], \";\", tf.math.real(kahler_metric[i]), \";\", tf.math.imag(kahler_metric[i])], summarize=-1, output_stream=\"file://\"+file_name)\n",
        "    print(f\"Done! Results in {file_name}\")\n",
        "\n",
        "    # Convert 2x2 complex to 4x4 real Hermitian?\n",
        "    # See \"Riemannian metric.ipynb\"\n",
        "\n",
        "    #eigvals = tf.linalg.eigh(M)[0]\n",
        "    #eigvals_np = eigvals.numpy()\n",
        "\n",
        "    return volume_form / factor"
      ],
      "metadata": {
        "id": "lLl2MXn05J0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Fubini_potential(tf.keras.Model):\n",
        "    def __init__(self, trainable=True, dtype='float32', **kwargs):\n",
        "        super(Fubini_potential, self).__init__(trainable=trainable, dtype=dtype, **kwargs)\n",
        "        self.abs_layer = tf.keras.layers.Lambda(lambda x: tf.abs(x))\n",
        "        self.layer1 = bnn.SquareDense(len(Z), len(Z), activation=tf.square)\n",
        "        self.layer2 = bnn.SquareDense(len(Z), 1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.abs_layer(inputs)  # Convert complex64 to float32 by taking the absolute value\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = tf.math.log(x)\n",
        "        return x\n",
        "\n",
        "fubi = Fubini_potential()\n",
        "\"\"\"\n",
        "fubi.layer1.w.assign(tf.constant([[1,0,0],[0,1,0],[0,0,1]], dtype=tf.float32))\n",
        "fubi.layer2.w.assign(tf.constant([[1],[1],[1]], dtype=tf.float32))\n",
        "\"\"\"\n",
        "fubi.layer1.w.assign(tf.constant([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]], dtype=tf.float32))\n",
        "fubi.layer2.w.assign(tf.constant([[1],[1],[1],[1]], dtype=tf.float32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riPaV6Yl5mQt",
        "outputId": "65c2ac53-4594-43d4-c4cd-c922fe03031a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
              "array([[1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_quotient(y_numerator, y_denominator, mass):  # mass-weighted\n",
        "    weights = mass / tf.reduce_sum(mass)\n",
        "    return tf.reduce_sum(tf.abs(y_numerator / y_denominator) * weights)\n",
        "\n",
        "def calculate_volume(dataset):\n",
        "    total_holo_volume = tf.constant(0, dtype=tf.float32)\n",
        "    total_kaehler_volume = tf.constant(0, dtype=tf.float32)\n",
        "    total_fubi_volume = tf.constant(0, dtype=tf.float32)\n",
        "    total_mass = tf.constant(0, dtype=tf.float32)\n",
        "    num_points = 0\n",
        "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(dataset):\n",
        "        det_fubi = volume_form_with_model(points, Omega_Omegabar, mass, restriction, fubi)\n",
        "        det_omega = volume_form_with_model(points, Omega_Omegabar, mass, restriction, model)\n",
        "        mass_sum = tf.reduce_sum(mass)\n",
        "        total_holo_volume += compute_quotient(Omega_Omegabar, det_fubi, mass) * mass_sum\n",
        "        total_kaehler_volume += compute_quotient(det_omega, det_fubi, mass) * mass_sum\n",
        "        total_fubi_volume += compute_quotient(det_fubi, det_fubi, mass)\n",
        "        total_mass += mass_sum\n",
        "        num_points += len(points)\n",
        "    total_holo_volume = total_holo_volume / total_mass\n",
        "    total_kaehler_volume = total_kaehler_volume / total_mass\n",
        "\n",
        "    return num_points, total_mass.numpy(), total_holo_volume.numpy(), total_kaehler_volume.numpy(), total_fubi_volume.numpy()\n",
        "\n",
        "num_points, total_mass, holo_vol, kaehler_vol, fubi_volume = calculate_volume(test_set)\n",
        "print(f'Num points: {num_points}; Total mass: {total_mass}; Holomorphic volume: {holo_vol}; Kahler volume: {kaehler_vol}; Fubi volume: {fubi_volume}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhdIJZre5gFp",
        "outputId": "ee1dd5f3-fe62-4046-bc2e-aeb1c68947ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Results in metric_k3_wt.csv\n",
            "Done! Results in metric_k3_wt.csv\n",
            "Num points: 40960; Total mass: 404437.59375; Holomorphic volume: 123.03026580810547; Kahler volume: 23.096782684326172; Fubi volume: 40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_quotient(y_numerator, y_denominator, mass): # batch-average\n",
        "    weights = mass / tf.reduce_sum(mass)\n",
        "    return tf.reduce_sum(tf.abs(y_numerator / y_denominator) * weights)\n",
        "\n",
        "def calculate_volume(dataset):\n",
        "    total_holo_volume = tf.constant(0, dtype=tf.float32)\n",
        "    total_kaehler_volume = tf.constant(0, dtype=tf.float32)\n",
        "    total_fubi_volume = tf.constant(0, dtype=tf.float32)\n",
        "    total_mass = tf.constant(0, dtype=tf.float32)\n",
        "    num_points = 0\n",
        "    for step, (points, Omega_Omegabar, mass, restriction) in enumerate(dataset):\n",
        "        det_fubi = volume_form_with_model(points, Omega_Omegabar, mass, restriction, fubi)\n",
        "        det_omega = volume_form_with_model(points, Omega_Omegabar, mass, restriction, model)\n",
        "        mass_sum = tf.reduce_sum(mass)\n",
        "        total_holo_volume += compute_quotient(Omega_Omegabar, Omega_Omegabar, mass)\n",
        "        total_kaehler_volume += compute_quotient(det_omega, Omega_Omegabar, mass)\n",
        "        total_fubi_volume += compute_quotient(det_fubi, Omega_Omegabar, mass)\n",
        "        total_mass += mass_sum\n",
        "        num_points += len(points)\n",
        "    total_holo_volume = total_holo_volume / total_mass\n",
        "    total_kaehler_volume = total_kaehler_volume / total_mass\n",
        "\n",
        "    return num_points, total_mass.numpy(), total_holo_volume.numpy(), total_kaehler_volume.numpy(), total_fubi_volume.numpy()\n",
        "\n",
        "num_points, total_mass, holo_vol, kaehler_vol, fubi_volume = calculate_volume(test_set)\n",
        "print(f'Num points: {num_points}; Total mass: {total_mass}; Holomorphic volume: {holo_vol}; Kahler volume: {kaehler_vol}; Fubi volume: {fubi_volume}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yam1-qw3txU1",
        "outputId": "f825ba68-a325-4fbb-9de0-7c78e2de9102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num points: 40960; Total mass: 404437.5625; Holomorphic volume: 9.890278306556866e-05; Kahler volume: 0.00022784460452385247; Fubi volume: 40.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('metric_k3_wt.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2wFXFYU1YbaB",
        "outputId": "f5ed2379-22a5-4b2a-e73e-6ce760649db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_19eed47f-a5bf-4f2c-9e70-d5dcfdb6d881\", \"metric_k3_wt.csv\", 5438151)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V6E1",
      "collapsed_sections": [
        "ilHaPYnkEi-S",
        "EPKhszPl96Am"
      ],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}